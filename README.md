Результаты выполнения заданий курса ‘Продвинутый Python’

В данном курсе были рассмотрены 16 заданий.

Задание 1. Импорт данных.

Условие: Возьмите данные по вызовам пожарных служб в Москве за 2015-2019 годы: https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv.  Получите из них фрейм данных (таблицу значений). По этому фрейму вычислите среднее значение вызовов пожарных  машин в месяц в одном округе Москвы, округлив до целых/

Решение: Импортируем pandas, считываем файл с помощью read_csv и выводим на печать среднее значение по столбцу ‘Calls’, округлив до целых с помощью round.


Задание2.  Данные из нескольких источников.

Условие: Получите данные по безработице в Москве: https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv. Объедините эти данные индексами (Месяц/Год) с данными из предыдущего задания (вызовы пожарных) для Центрального административного округа: https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv. Найдите значение поля UnemployedMen в том месяце, когда было меньше всего вызовов в Центральном дминистративном округе.

Решение: Импортируем pandas, считываем данные по безработице с помощью read_csv и выставляем индексы по столбцам  ‘Year’ и  ‘Period’. Далее считываем также с помощью read_csv данные по вызовам пожарных служб в Москве за 2015-2019 годы и выставляем индексы 'AdmArea', 'Year' и 'Month'.Используя метод  loc[] выбираем только данные  для Центрального административного  округа. Переназначим названия колонок во втором наборе данных вместо названия ‘Month' название‘Period’, после этого объединяем два набора данных по индексам с помощью метода merge(). Далее отбрасываем индексы через reset_index() и назначаем индексы по столбцу 'Calls'. Сортируем данные по этому индексу и выводим на печать первые данные в столбце 'UnemployedMen'. Это и будет наш ответ.


Задание 3: Выделение данных.

Условие: Получите данные по безработице в Москве: https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv. Найдите, с какого года процент людей с ограниченными возможностями (UnemployedDisabled) среди всех безработных (UnemployedTotal) стал меньше 2%.

Решение: Импортируем pandas, считываем данные по безработице с помощью read_csv. Создаем столбец 'Percent', где будут отображаться процент людей с ограниченными возможностями среди всех безработных (data['UnemployedDisabled'] * 100 / data['UnemployedTotal']). После этого выберем данные, где процент людей с ограниченными возможностями менее 2. Назначаем индекс по году (‘Year’) и сортируем данные по этому индексу. Выводим на печать первый индекс, это и будет год, с которого процент людей с ограниченными возможностями среди всех безработных стал меньше 2%.


Задание 4: Предсказание на 2020 год

Условие: Возьмите данные по безработице в городе Москва: video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv. Сгруппируйте данные по годам, и, если в году меньше 6 значений, отбросьте эти годы. Постройте модель линейной регрессии по годам среднего значения отношения UnemployedDisabled к UnemployedTotal (процента людей с ограниченными возможностями) за месяц и ответьте, какое ожидается значение процента безработных инвалидов в 2020 году при сохранении текущей политики города Москвы? Ответ округлите до сотых. 

Решение: Подключаем необходимые библиотеки: numpy, pandas и модуль LinearRegression из sklearn.linear_model. Считываем данные по безработице с помощью read_csv.  Создаем столбец 'Percent', где будут отображаться процент людей с ограниченными возможностями среди всех безработных (data['UnemployedDisabled'] * 100 / data['UnemployedTotal']). Далее группируем данные по году, используя метод groupby, отбросив данные, где количество данных менее пяти, используя метод filter() и лямда-функцию. После таких преобразований, заново группируем через groupby данные по году и берем их средние значения (mean). Далее приводим индексы группы данных к массиву (array()  –  одномерный массив, а reshare () –  уже двухмерных массив) для оси X по годам и для оси Y по столбцу 'Percent'. Загружаем модель линейной регрессии с нашими данными с помощью метода fit() и выводим предсказанное значение на 2020 год, округляя значение с помощью round()  до сотых процента.


Задание 5: Получение данных по API.

Условие: Изучите API Геокодера Яндекса tech.yandex.ru/maps/geocoder/doc/desc/concepts/input_params-docpage/ и получите ключ API для него в кабинете разработчика.  Выполните запрос к API и узнайте долготу точки на карте (Point) для города Самара.

Решение: Переходим на страницу геокодера, получаем ключ API для разработчика, заполняем geocode=Самара, ключ API (apikey) и format=json и получаем следующий GET-запрос: https://geocode-maps.yandex.ru/1.x/?format=json&apikey=3f355b88-81e9-4bbf-a0a4-eb687fdea256&geocode=Самара'. Далее подключаем необходимые библиотеки requests и json. Отправляем GET-запрос к API, далее загружаем ответ от GET-запроса через json.loads(), далее последовательно сокращая наш объект, доходим до искомого значения. 


Задание 6: Получение котировок акций.

Условие: Получите данные по котировкам акций со страницы: mfd.ru/marketdata/?id=5&group=16&mode=3&sortHeader=name&sortOrder=1&selectedDate=01.11.2019 и найдите, по какому тикеру был максимальный рост числа сделок (в процентах) за 1 ноября 2019 года.

Решение: Подключаем библиотеку requests, pandas и модуль BeautifulSoup из библиотеки bs4, получаем страницу с помощью requests.get, и получаем из нее html-код, используя BeautifulSoup. Далее находим  в html-коде таблицу по тегу table и id=’marketDataList’. Далее разбираем таблицу, находим тэги tr и с помощью цикла в строчку tr добавляем данные из тэгов td, используя strip=True  для того, чтоб избавится от ненужных пробелов, символов переноса строки и т.д. Если длина строк будет больше, то мы добавляем в строку данные  из tr. Далее загружаем в DataFrame  полученные данные и назовем столбцы соответствующими названиями, исходя из названий в таблице на странице. Далее фильтруем данные, чтоб у нас в данных отсутствовали сделки с ‘N/A’. После этого приводим столбец ‘C%’ к математическому виду(убираем знак %, и меняем знак дефиса на знак минуса) и с помощью метода astype привести данные к числу float() . Далее выставляем индекс с помощью set_index и сортируем данные в порядке убывания. Выводим данные , которые будут находиться в первой строчке в столбце ‘Тикер’.


Задание 7: Парсинг интернет-магазина.

Условие: Используя парсинг данных с маркетплейса beru.ru, найдите, на сколько литров отличается общий объем холодильников Саратов 263 и Саратов 452? Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу: video.ittensive.com/data/018-python-advanced/beru.ru.

Решение: : Подключаем библиотеку requests  и модуль BeautifulSoup из библиотеки bs4. Согласно этике парсинга, создаем словарь headers с ключем "User-Agent" и значением "ittensive-python-courses/1.0 (+https://www.ittensive.com)". Далее получаем страницу с помощью requests.get, не забыв указать параметр headers, и получаем из нее html-код, используя BeautifulSoup. После этого находим все ссылки с тегом <a> и с классом "grid-snippet__react-link", используя find_all.  После поиска всех ссылок, будем искать все ссылки на искомые холодильники. Если ссылки были найдены, тогда мы можем получить данные из этих ссылок, для этого создадим функцию, которая находит объем холодильной камеры, для этого загружаем данные из ссылок, находим html с помощью BeatifulSoup, ищем в коде объем холодильной камеры (находится в теге 'span' с классом ‘_112Tad-7AP’) и с помощью генератора находим все цифры и объединив все найденные цифры через .join(), возвращаем эти данные, переведя их в целое число (int). Далее с помощью данной функции мы находим объемы холодильных камер искомых холодильников. Т.к. объемы заранее нам не известны, поэтому находим абсолютное значение разности объемов холодильных камер искомых холодильников. После этого с помощью f-строки выводим наш ответ.


Задание 8: Загрузка результатов в БД.

Условие: Соберите данные о моделях холодильников Саратов с маркетплейса beru.ru: URL, название, цена, размеры, общий объем, объем холодильной камеры. Создайте соответствующие таблицы в SQLite базе данных и загрузите полученные данные в таблицу beru_goods. Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу:
video.ittensive.com/data/018-python-advanced/beru.ru/

Решение: Подключаем библиотеки sqlite3, requests  и модуль BeautifulSoup из библиотеки bs4. Создадим вспомогательную функцию find_data для поиска ссылок.  Для этого загружаем страницу с помощью requests.get и получаем из нее html-код, используя BeautifulSoup.  Ищем данные для названия холодильника title (тег h1),  цены price ("span", {"data-tid": "c3eaad93") и параметров товара tags ("span", {"class": "_112Tad-7AP"}), и заводим переменные под эти параметры (длина, ширина, глубина, общий объем холодильника и объем морозильной камеры ). Перебирая параметры tags, вычленяем нужные параметры.  Для этого нам понадобится дополнительная функция find_number, которая из строки будет вычленять числа. С помощью данной функции находим общий объем холодильника и объем морозильной камеры. Перебрав все параметры tags, возвращаем  параметры: длину, ширину, глубину, общий объем холодильника и объем морозильной камеры. После поиска всех ссылок, будем искать все ссылки на холодильники ‘Саратов’, добавляя найденные холодильники в data. После того, как вспомогательные функции готовы, берем основную страницу с помощью requests.get и получаем из нее html-код, используя BeautifulSoup.  После этого находим все ссылки с тегом <a> и с классом "grid-snippet__react-link", используя find_all. Заводим массив data и перебираем все ссылки. Если в ссылке есть название ‘Саратов’, тогда в массив добавляем данные, используя функцию find_data. Когда все данные готовы, создаем подключение  к базе данных conn, устанавливаем соединение cursor() и создаем таблицу с помощью запроса: CREATE table beru_goods, и в этом запросе передаем все требуемые поля: id, url, title, price, width, depth, height, volume и freezer. После этого переносим наши данные в таблицу, для этого передаем запрос INSERT INTO beru_goods, не забывая после каждого запроса применять изменения с помощью команды commit(). После загрузки всех данных выводим данные на печать с помощью запроса ("SELECT * FROM beru_goods").fetchall() и закрываем соединение с помощью close().


Задание 9: Тип визуализации данных.

Условие: Загрузите данные по ЕГЭ за последние годы https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv выберите данные за 2018-2019 учебный год. Выберите тип диаграммы для отображения результатов по административному округу Москвы, постройте выбранную диаграмму для количества школьников, написавших ЕГЭ на 220 баллов и выше. Выберите тип диаграммы и постройте ее для районов Северо-Западного административного округа Москвы для количества школьников, написавших ЕГЭ на 220 баллов и выше.

Решение: Импортируем библиотеки pandas и matplotlib.pyplot, загружаем данные с помощью read_csv. Далее в столбце ‘District’ в названиях уберем слово ‘район’ и назначим как категорию с помощью метода astype('category'). В столбце ‘AdmArea’ с помощью лямда-функции разбиваем название на слова с помощью split() и оставляем в столбце только первое слово. Выставляем индекс по году и получаем данные только за 2018-2019 года, и сбрасываем индекс, используя методы set_index(), loc[] и reset_index().  Далее выводим две круговые диаграммы: сначала выводим заголовок ‘ЕГЭ по округам’, назначаем индекс по данным ‘AdmArea’, и сгруппируем данные количества школьников, написавших ЕГЭ на 220 баллов с общим количеством учеников и выведем сумму на круговой диаграмме.  Для второй диаграмме выберем данные в Северо-Западном округе (метод loc[]), добавляем область area для графика, выводим заголовок title ‘ЕГЭ в Северо-Западном округе’, сбрасываем предыдущий индекс и назначаем новый ‘District’. Отсекаем только отличников и строим круговую диаграмму по району. Вычисляем общее число отличников по району и через лямду-функцию сначала округляем значение до сотых и после этого приводим значения к целому числу, после этого выводим наши графики.


Задание 10: Результаты марафона.

Условие: Загрузите данные по итогам марафона https://video.ittensive.com/python-advanced/marathon-data.csv. Приведите время половины и полной дистанции к секундам. Найдите, данные каких серии данных коррелируют (используя диаграмму pairplot в Seaborn). Найдите коэффициент корреляции этих серий данных, используя scipy.stats.pearsonr. Постройте график jointplot для коррелирующих данных.

Решение: Импортируем библиотеки pandas, seaborn, matplotlib.pyplot и scipy.stats, загружаем данные с помощью read_csv. Далее для столбцов 'split' и 'final' переведем данные из формата чч:мм:сс в секунды, воспользовавшись лямда-функцией. Далее построим парный график, для групп используем пол спортсменов. На основании построенных графиков видно, что коррелируют только данные ‘final’ и ‘split’. Строим корреляционный график для этих двух переменных (joinplot) и находим коэффициент Пирсона, и выводим его на печать.


Задание 11: Скользящие средние на биржевых графиках.

Условие: Используя данные индекса РТС за последние годы https://video.ittensive.com/python-advanced/rts-index.csv, постройте отдельные графики закрытия (Close) индекса по дням за 2017, 2018, 2019 годы в единой оси X. Добавьте на график экспоненциальное среднее за 20 дней для значения Max за 2017 год. Найдите последнюю дату, когда экспоненциальное среднее максимального дневного значения (Max) в 2017 году было больше, чем соответствующее значение Close в 2019 году (это последнее пересечение графика за 2019 год и графика для среднего за 2017 год).

Решение: Импортируем библиотеки pandas, matplotlib.pyplot и модуль rcParams из библиотеки matplotlib, загружаем данные с помощью read_csv. Преобразовываем данные из столбца ‘Date’ из строки в дату с помощью to_datetime и переведем его в формат гггг-мм-дд с помощью dayfirst=True. Зададим диапазон дат с помощью метода date_range и проиндексируем по дате с помощью метода set_index(). После этого переиндексируем с помощью reindex  и заполним пустые данные предыдущими значениями, используя ffill().  Добавим серию ‘Day’ для подписи по оси Х, назначим название индекса ‘Date’ и отсортируем данные по этому индексу. Далее создадим два набора данных: за 2019 и за 2017 года, выбрав нужный год с помощью loc() и переназначив индекс по дню. Строим графики за 2019,экспонициальное среднее со сдвигом 20 за 2017 год,  за 2017 и за 2018 года на одном холсте. Добавляем легенду. Находим точную дату, когда экспоненциальное среднее максимального дневного значения (Max) в 2017 году было больше, чем соответствующее значение Close в 2019 году, для этого сначала отфильтруем данные за 2019 год, когда закрытие дня было больше максимума аналогичного дня за 2017 год. Выставляем индекс по дате, сортируем данные по убыванию и находим наш ответ.


Задание 12: Объекты культурного наследия в России.

Условие: Изучите набор данных по объектам культурного наследия России (в виде gz-архива): https://video.ittensive.com/python-advanced/data-44-structure-4.csv.gz и постройте фоновую картограмму по количеству объектов в каждом регионе России, используя гео-данные https://video.ittensive.com/python-advanced/russia.json. Выведите для каждого региона количество объектов в нем. Посчитайте число объектов культурного наследия в Татарстане.

Решение: Импортируем библиотеки matplotlib.pyplot, geopandas, pandas и descartes, загружаем данные с помощью read_csv. Приводим данные ‘Регион’ к верхнему регистру (upper()), группируем по региону (groupby) и посчитаем число объектов по каждому региону (count()). Загружаем геоданные с помощью geopandas. read_file, переименовываем регионы для слияния с первым набором данных (replace()) и переходим к слиянию наборов данных (merge()) по региону.
После этого создаем холст, область отрисовки и нанесём картограмму. Отсечем Чукотку и все, что западнее Калининграда. Нанесем аннотацию в виде числа объектов в каждом регионе, перебрав все строки в итоговом наборе данных. Выводим картограмму , а также число объектов культурного наследия в Татарстане.


Задание 13: Сборка PDF документа.

Условие: Используя данные по посещаемости библиотек в районах Москвы https://video.ittensive.com/python-advanced/data-7361-2019-11-28.utf.json, постройте круговую диаграмму суммарной посещаемости (NumOfVisitors) 20 наиболее популярных районов Москвы.
Создайте PDF отчет, используя файл https://video.ittensive.com/python-advanced/title.pdf как первую страницу. На второй странице выведите итоговую диаграмму, самый популярный район Москвы и число посетителей библиотек в нем.

Решение: Импортируем библиотеки: reportlab, PyPDF2, requests , json , pandas, matplotlib.pyplot и seaborn. Загружаем данные через requests и передаем в фрейм данных через DataFrame, заполняя отсутствующие данные нулями (fillna(value=0)). Т.к. название районов скрыто в словарь 'ObjectAddress', создадим отдельную функцию выявления районов из данных и создадим столбец 'District' с помощью данной функции, группируем по данному столбцу и сортируем в порядке убывания по данным 'NumOfVisitors'. Создаем холст, строим круговую диаграмму 20 самых популярных районов, убрав Labels, и создаем легенду. Сохраняем диаграмму в формате ‘png’.
Cоздаем pdf-документ, задав шрифт, размер холста, вставляем диаграмму, выводим название диаграммы, самого популярного района Москвы и количество посетителей. После объединяем титульный лист с созданным ранее PDF-документом с помощью PdfFileMerger и получаем PDF отчет.


Задание 14: Геральдические символы Москвы.

Условие: Сгенерируйте PDF документ из списка флагов и гербов районов Москвы: https://video.ittensive.com/python-advanced/data-102743-2019-11-13.utf.csv. На каждой странице документа выведите название геральдического символа (Name), его описание (Description) и его изображение (Picture).  Для показа изображений используйте адрес: https://op.mos.ru/MEDIA/showFile?id=XXX, где XXX - это значение поля Picture в наборе данных. Например: https://op.mos.ru/MEDIA/showFile?id=8466da35-6801-41a9-a71e-04b60408accb.

Решение: Импортируем библиотеки pandas и pdfkit, считываем файл с помощью read_csv и формируем html-документ, используя теги html, head, title, body, h1. Для постраничного вывода данных используем цикл for …in …, где для первого элемента не задаем стиль, а для всех остальных вставим разрыв страницы. После вывода заголовка выведем изображение геральдического символа с помощью тега img, используя адрес изображения с значением item['Picture']  и добавим описание геральдического символа, используя данные item['Description']. Далее с помощью pdfkit формируем PDF-документ из списка флагов и гербов районов Москвы.


Задание 15: Многостраничный отчет.

Условие: Используя данные по активностям в парках Москвы https://video.ittensive.com/python-advanced/data-107235-2019-12-02.utf.json.  Создайте PDF отчет, в котором выведите:
1. Диаграмму распределения числа активностей по паркам, топ10 самых активных
2. Таблицу активностей по всем паркам в виде Активность-Расписание-Парк.
3. Сколько активностей Тайцзицюань есть в парках Москвы?

Решение: Импортируем библиотеки: requests , json , pandas, matplotlib.pyplot, pdfkit, BytesIO и binascii. Загружаем данные, формируем данные из колонок "CourseName", "CoursesTimetable" и "NameOfPark". В столбце "NameOfPark" с помощью лямда-функции оставляем только значение "value", переименовываем колонки в "Активность", "Расписание" и "Парк". Выводим на печать активности Тайцзицюань в парках Москвы. Создаем холст, группируем данные по столбцам "Парк" и отсортируем по убыванию данных "Активности". Далее строим круговую диаграмму 10 наиболее “активных” парков с помощью plot.pie.  Далее сохраняем график с помощью BytesIO и преобразовываем бинарные данные в формат base64 и декодируем данные в UTF-8. Чтобы pandas не ограничивал длину данных в ячейках , настроим через set_option длину данных до 1000 символов. Далее формируем HTML-отчет, используя теги html, head, title, body, h1 и img src=, после чего с помощью pdfkit формируем PDF-отчет с диаграммой распределения числа активностей по паркам  таблицей активностей по всем паркам.


Задание 16: Автоматические отчеты.

Условие: Соберите отчет по результатам ЕГЭ в 2018-2019 году, используя данные https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv и отправьте его в HTML формате по адресу support@ittensive.com, используя только Python. В отчете должно быть:
•	общее число отличников (учеников, получивших более 220 баллов по ЕГЭ в Москве),
•	распределение отличников по округам Москвы,
•	название школы с лучшими результатами по ЕГЭ в Москве.
Диаграмма распределения должна быть вставлена в HTML через data: URL формат (в base64-кодировке). Дополнительно: приложите к отчету PDF документ того же содержания (дублирующий письмо).

Решение: Импортируем библиотеки: pandas, matplotlib.pyplot, pdfkit, BytesIO, binascii, smtplib, encoders и модули email:  MIMEText, MIMEBase и MIMEMultipart. Считываем данные с помощью read_csv, выделяем данные за 2018-2019 года. Находим лучшую школу по результатам ЕГЭ, для этого сортируем данные по столбцу "PASSES_OVER_220" и берем первые данные. Далее группируем по административному округу ("PASSES_OVER_220"), сократив название с помощью лямда-функции, разбив название на слова с помощью split()и взяв из названия округов только первое слово. Группируем данные "AdmArea" и "PASSES_OVER_220" для построения графиков. После этого создаем круговую диаграмму по округам (plot.pie) , выводим названия и легенду. Далее сохраняем изображение с помощью BytesIO. После этого подготовим изображение для отчета и переведем его в base64. Следующим этапом было создание html-отчета, после его создания с помощью pdfkit сформируем PDF-отчет. Создаем объект MIMEMultipart, создаем поля "From", "Subject",  "Content-Type" и "To". В тело письма загрузим HTML-документ и вложим PDF-отчет. Далее подключились к серверу и отправили письмо. Отключаем сервер.
